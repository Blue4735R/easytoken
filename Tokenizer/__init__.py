
from Tokenizer.tokenizer import WordTokenizer
from Tokenizer.tokenizer import SentenceTokenizer